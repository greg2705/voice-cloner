{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from TTS.tts.configs.xtts_config import XttsConfig # type: ignore\n",
    "# from TTS.tts.models.xtts import Xtts # type: ignore\n",
    "# import torch\n",
    "\n",
    "# config = XttsConfig()\n",
    "# config.load_json(\"Model\\XTTS_v2\\config.json\")\n",
    "# model = Xtts.init_from_config(config)\n",
    "# model.load_checkpoint(config, checkpoint_dir=\"Model\\XTTS_v2\", eval=True)\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# if (device==\"cuda\"):\n",
    "#     print(\"Using GPU\")\n",
    "# else:\n",
    "#     print(\"Using CPU\")\n",
    "\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils_gradio\n",
    "\n",
    "# utils_gradio.get_audio_from_video(\"https://www.youtube.com/watch?v=BLLqu5azr7s&t\",\"Upload_temp/\",\"finetuning_test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Greg\\anaconda3\\envs\\VoiceCloner\\lib\\site-packages\\pyannote\\audio\\core\\io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\Greg\\.cache\\torch\\whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.3.1+cu118. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "import stt_pipeline\n",
    "import os\n",
    "\n",
    "res=stt_pipeline.stt(\"Model/faster_whisper_v3/\",\"Upload_Temp/finetuning_test.wav\",\"fr\",device=\"cuda\",batch_size=12,compute_type=\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Greg\\anaconda3\\envs\\VoiceCloner\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Greg\\anaconda3\\envs\\VoiceCloner\\lib\\site-packages\\pyannote\\audio\\core\\io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"C:/Users/Greg/Documents/Job/PythonProject/VoiceCloner/Model/models/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Greg\\anaconda3\\envs\\VoiceCloner\\lib\\site-packages\\pyannote\\audio\\core\\io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb5f943f0584701a5ce1f54e0debd3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import stt_pipeline\n",
    "# res=stt_pipeline.diarization(\"Model/models/config.yaml\",\"Upload_Temp/finetuning_test.wav\",device=\"cuda\")\n",
    "\n",
    "# from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "# from pyannote.audio import Pipeline\n",
    "# import torch\n",
    "\n",
    "# pipeline = Pipeline.from_pretrained(\"Model/models/config.yaml\")\n",
    "# pipeline.to(torch.device(\"cuda\"))\n",
    "\n",
    "# from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "\n",
    "# with ProgressHook() as hook:\n",
    "#     diarization = pipeline(\"Upload_Temp/finetuning_test.wav\", hook=hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\Greg\\.cache\\torch\\whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import gradio as gr\n",
    "# from tqdm import tqdm\n",
    "# from typing import Any, Mapping, Optional, Text\n",
    "# from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "# from pyannote.audio import Pipeline\n",
    "# import torch\n",
    "# import gc \n",
    "# import torch \n",
    "# import stt_pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import gradio as gr\n",
    "# import time\n",
    "# import threading\n",
    "# import time\n",
    "# def my_function(x, progress=gr.Progress(track_tqdm=True)):\n",
    "#     stt_pipeline.load_audio_tqdm(\"Model/faster_whisper_v3\",\"Model/models/config.yaml\",\"Upload_Temp/40_min.wav\",\"fr\",device=\"cuda\",batch_size=6,compute_type=\"int8\")\n",
    "#     return x\n",
    "# gr.Interface(my_function, gr.Textbox(), gr.Textbox()).queue().launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Greg\\anaconda3\\envs\\VoiceCloner\\lib\\site-packages\\pyannote\\audio\\core\\io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "Step 1/3: segmentation: 100%|██████████| 1052/1052 [00:02<00:00, 495.30it/s]\n",
      "Step 1/3: speaker_counting: 0it [00:00, ?it/s]\n",
      "Step 2/3: embeddings: 100%|██████████| 263/263 [01:27<00:00,  3.01it/s]\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from pyannote.audio import Pipeline\n",
    "import stt_pipeline\n",
    "\n",
    "def extract_speaker_audio(annotation, audio_path):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    \n",
    "    # Initialize the dictionary to store speaker audio segments\n",
    "    speaker_audio = {}\n",
    "    \n",
    "    # Iterate over the segments in the annotation\n",
    "    for segment, _, speaker in annotation.itertracks(yield_label=True):\n",
    "        start_time = segment.start\n",
    "        end_time = segment.end\n",
    "        \n",
    "        # Extract the audio segment for the current speaker\n",
    "        start_sample = int(start_time * sr)\n",
    "        end_sample = int(end_time * sr)\n",
    "        audio_segment = y[start_sample:end_sample]\n",
    "        \n",
    "        # Add the audio segment to the dictionary\n",
    "        if speaker not in speaker_audio:\n",
    "            speaker_audio[speaker] = []\n",
    "        speaker_audio[speaker].append(audio_segment)\n",
    "    \n",
    "    # Concatenate all segments for each speaker and filter by duration\n",
    "    filtered_speaker_audio = {}\n",
    "    for speaker in speaker_audio:\n",
    "        concatenated_audio = np.concatenate(speaker_audio[speaker])\n",
    "        duration = len(concatenated_audio) / sr\n",
    "        if duration >= 120:\n",
    "            filtered_speaker_audio[speaker] = (concatenated_audio, sr)\n",
    "    \n",
    "    return filtered_speaker_audio\n",
    "\n",
    "\n",
    "audio_path=\"C:/Users/Greg/Downloads/mbappe.mp3\"\n",
    "anno=stt_pipeline.diarization(\"Model/models/config.yaml\",audio_path,device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SPEAKER_01': (array([-0.01800936, -0.03317791, -0.02888568, ..., -0.00330049,\n",
       "         -0.00289264, -0.00582846], dtype=float32),\n",
       "  44100)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r=extract_speaker_audio(anno, audio_path)\n",
    "r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
