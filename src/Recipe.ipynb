{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook provides user-friendly code for several speech processing tasks: text-to-speech (TTS), speech-to-text (STT), speaker diarization, and finetuning a TTS model. The relevant Python scripts are located in the utils folder.\n",
    "\n",
    "#### To get started, we recommend using a Poetry environment for managing dependencies. For optimal performance, it is advisable to have an NVIDIA GPU with CUDA drivers installed. However, if a GPU is not available, you can still run TTS and STT at a reasonable speed on a CPU. Additionally, some implementations, such as whispercpp and ONNX, offer hardware acceleration for CPU usage.\n",
    "## I) One shot cloning with TTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.gradio_helpers import download_model_hf\n",
    "\n",
    "## Download model from HF if not already download, customize the paths\n",
    "where_to_dl=\"xtts_v2\"\n",
    "model_to_dl=\"coqui/XTTS-v2\"\n",
    "download_model_hf(where_to_dl,model_to_dl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tts_pipeline import load_model\n",
    "\n",
    "## Load the model\n",
    "model=load_model(where_to_dl)\n",
    "\n",
    "import torch \n",
    "## Send to the GPU if cuda is avaible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "if device == \"cuda\":\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tts_pipeline import generation,load_speaker, save_speaker\n",
    "\n",
    "## We are gonna do the voice cloning using one shot method\n",
    "##Your audio path you want to clone, using a high quality audio between 6-12 seconds is recommended\n",
    "## Here I will use audio from https://huggingface.co/coqui/XTTS-v2/tree/main/samples (wav, mp3 ... are working)\n",
    "\n",
    "audio_path=\"samples_en_sample.wav\" \n",
    "\n",
    "## First we compute the speaker_embedding and gpt_cond_latent\n",
    "\n",
    "gpt_cond_latent, speaker_embedding = model.get_conditioning_latents(audio_path=audio_path)\n",
    "\n",
    "## You can save the gpt_cond_latent, speaker_embedding to load it faster using the following code\n",
    "\n",
    "# save_speaker(model,audio_path,\"your\\path\\to\\save\")\n",
    "# gpt_cond_latent, speaker_embedding = load_speaker(\"your\\path\\to\\save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Function Arguments\n",
    "\n",
    "The generation function takes a lot of arguments. Here is a detailed list:\n",
    "\n",
    "#### Required Arguments\n",
    "\n",
    "- **model**: \n",
    "  - Must be `xtts_v2` or a fine-tuning of `xtts_v2`.\n",
    "  \n",
    "- **text**: \n",
    "  - The text expected in the audio.\n",
    "\n",
    "- **language**: \n",
    "  - The language of the text. \n",
    "  - This argument doesn't depend on the input audio and can be chosen among 17 languages:\n",
    "    - English (`en`)\n",
    "    - Spanish (`es`)\n",
    "    - French (`fr`)\n",
    "    - German (`de`)\n",
    "    - Italian (`it`)\n",
    "    - Portuguese (`pt`)\n",
    "    - Polish (`pl`)\n",
    "    - Turkish (`tr`)\n",
    "    - Russian (`ru`)\n",
    "    - Dutch (`nl`)\n",
    "    - Czech (`cs`)\n",
    "    - Arabic (`ar`)\n",
    "    - Chinese (`zh-cn`)\n",
    "    - Japanese (`ja`)\n",
    "    - Hungarian (`hu`)\n",
    "    - Korean (`ko`)\n",
    "    - Hindi (`hi`)\n",
    "\n",
    "- **gpt_cond_latent**:\n",
    "  - The gpt_cond_latent from the input audio\n",
    "  \n",
    "- **speaker_embedding**:\n",
    "  - The speaker_embedding from the input audio\n",
    "\n",
    "- **output_path**\n",
    "  - Where to save the generated audio\n",
    "\n",
    "### Optional Arguments\n",
    "\n",
    "- **temperature**: \n",
    "  - The softmax temperature of the autoregressive model.\n",
    "  - Defaults to `0.65`.\n",
    "\n",
    "- **speed**: \n",
    "  - The speed rate of the generated audio.\n",
    "  - Set it between `0` and `2`.\n",
    "  - Default is `1.0`.\n",
    "\n",
    "- **Repetition Penalty**: \n",
    "  - A penalty that prevents the autoregressive decoder from repeating itself during decoding.\n",
    "  - Can be used to reduce the incidence of long silences or “uhhhhhhs”, etc.\n",
    "  - Defaults to `2.0`.\n",
    "\n",
    "- **Top K**: \n",
    "  - Lower values mean the decoder produces more “likely” (aka boring) outputs.\n",
    "  - Defaults to `50`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "## First audio generated in english\n",
    "generation(model,\"My first audio generated\",\"en\",gpt_cond_latent,speaker_embedding,\"test.wav\")\n",
    "IPython.display.Audio(\"test.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can also do it another language, using french for example\n",
    "generation(model,\"Mon premier audio en français\",\"fr\",gpt_cond_latent,speaker_embedding,\"test_fr.wav\")\n",
    "IPython.display.Audio(\"test_fr.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can play with other parameters (more user friendly using the Gradio App)\n",
    "dict_args={\"speed\":0.7,\"top_k\":80,\"temperature\":0.8}\n",
    "generation(model,\"My second audio generated using different parameters\",\"en\",gpt_cond_latent,speaker_embedding,\"test_2.wav\",dict_args)\n",
    "IPython.display.Audio(\"test_2.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dump the model to save the vram\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Speech to Text & Speaker Diarization\n",
    "#### For the speech to text model we will use faster-whisper-large-v3, a faster implementation off whisper (if you are using a cpu you can use a smaller model). For all the notebook examples I will use a truncated podcast with Elon Musk(https://www.youtube.com/watch?v=JN3KPFbWCy8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_stt=\"faster_whisper\"\n",
    "download_model_hf(model_path_stt,\"Systran/faster-whisper-large-v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stt Function Arguments\n",
    "\n",
    "The `stt` function is used for converting speech to text. Here is a detailed list of its arguments:\n",
    "\n",
    "### Required Arguments\n",
    "\n",
    "- **model_path**: \n",
    "  - A string indicating the path to the model to be used for transcription.\n",
    "\n",
    "- **audio_file**: \n",
    "  - A string specifying the path to the audio file to be transcribed.\n",
    "\n",
    "- **lang**: \n",
    "  - A string representing the language of the audio. Whisper supports even more languages than tts.\n",
    "\n",
    "### Optional Arguments\n",
    "\n",
    "- **device**: \n",
    "  - Specifies the device to run the model on. Cuda will be faster\n",
    "  - Defaults to `\"cuda\"`.\n",
    "  - Can also be set to `\"cpu\"` if CUDA is not available.\n",
    "\n",
    "- **batch_size**: \n",
    "  - An integer indicating the number of audio samples to process in each batch. Lower if you don't have a lot of VRAM\n",
    "  - Defaults to `12`.\n",
    "\n",
    "- **compute_type**: \n",
    "  - Specifies the precision type for computations.\n",
    "  - Defaults to `\"float16\"`.\n",
    "  - Can be changed to `\"int8\"` for lower precision at the cost of performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.stt_pipeline import stt, stt_string\n",
    "audio_stt=\"elo_musk_podcast.wav\"\n",
    "lang=\"en\"\n",
    "res=stt(model_path_stt,audio_stt,lang,device,batch_size= 12,compute_type = \"int8\") #you lower batch_size if you have cuda oom errors\n",
    "print()\n",
    "print(stt_string(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the diarization model, we will need a hf token (you can grabe one here : https://huggingface.co/settings/tokens). Without GPU the diarization is relatively slow (even with the GPU).\n",
    "\n",
    "## Diarization Function Arguments\n",
    "\n",
    "The `diarization` function is used for speaker diarization in audio files. Here is a detailed list of its arguments:\n",
    "\n",
    "### Required Arguments\n",
    "\n",
    "- **model_path**: \n",
    "  - A string indicating the path to the diarization model to be used.\n",
    "\n",
    "- **audio_file**: \n",
    "  - A string specifying the path to the audio file for diarization.\n",
    "\n",
    "- **token_hf**: \n",
    "  - A string representing the token for authentication with the Hugging Face API or service.\n",
    "\n",
    "### Optional Arguments\n",
    "\n",
    "- **device**: \n",
    "  - Specifies the device to run the model on.\n",
    "  - Defaults to `\"cuda\"`.\n",
    "  - Can also be set to `\"cpu\"` if CUDA is not available.\n",
    "\n",
    "- **embedding_batch_size**: \n",
    "  - An integer indicating the number of audio segments to process in each batch during embedding computation.\n",
    "  - Defaults to `12`.\n",
    "\n",
    "- **segmentation_batch_size**: \n",
    "  - An integer indicating the number of audio segments to process in each batch during segmentation.\n",
    "  - Defaults to `12`.\n",
    "\n",
    "- **num speakers**:\n",
    "  - Number of speakers in the audio (the diarization will be more precise if provided)\n",
    "  - Default to -1 (same as unknown)\n",
    "  - This parameters can improve a lot the diarization\n",
    "\n",
    "### Return\n",
    "\n",
    "- **Annotation**:\n",
    "  - The function returns an `Pyannote.Annotation` object, which contains information about the different speakers identified in the audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.stt_pipeline import diarization\n",
    "HF_token=\"\" #Your HF token\n",
    "res_dia=diarization(\"pyannote/speaker-diarization-3.1\",\"elo_musk_podcast.wav\",token_hf=HF_token,device=device,embedding_batch_size=12,segmentation_batch_size=12)\n",
    "res_dia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### matching_stt_dia, stt_dia_str are homemade functions to match the transcriptiona and diarization. It's possible to improve them using punctuation analysis, LLMs ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from utils.stt_pipeline import matching_stt_dia, stt_dia_str\n",
    "\n",
    "res_stt_dia=matching_stt_dia(res,res_dia)\n",
    "print(stt_dia_str(res_stt_dia))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Finetuning TTS model\n",
    "#### This section will only works with GPU (nvidia).\n",
    "#### Everything is based from here https://docs.coqui.ai/en/latest/models/xtts.html#training\n",
    " - I) Dataset creation\n",
    " - II) Training\n",
    " - III) Inference\n",
    "\n",
    "\n",
    "### I) Dataset Creation\n",
    "#### See this post to know globally how the training dataset will be build (https://github.com/coqui-ai/TTS/wiki/What-makes-a-good-TTS-dataset), but keep in mind that audios with enough speech (between 20min - 2 hours for one speaker), no background noises (music, laughs) and good quality will always have better results.\n",
    "#### The audio in the training will be between 1 & 15 seconds, so the trick will be to split the audios you want to use for training. The dataset need to be in Coqui  Style Dataset which is :\n",
    "- The name of an audio file\n",
    "- The text for that file. E.g., \"Jane eyre by Charlotte Bronte. Chapter 1.\"\n",
    "- The speaker. E.g., \"speaker_01\". (optional)\n",
    "- The tone.  E.g., \"neutral\". (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.finetuning import suppress_overlaps_and_blanks\n",
    "HF_token=\"\" #Your HF token\n",
    "wave,sp_rate,res_dia=suppress_overlaps_and_blanks([\"elo_musk_podcast.wav\"],HF_token,num_speakers=2) #if memory errors you can, lower embedding_batch_size=12,segmentation_batch_size=12 in arguments\n",
    "res_dia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's choose speaker(s) we want to clone to create our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.finetuning import get_matching_audio_speakers, create_audio\n",
    "lst_speakers=[\"SPEAKER_01\"] #Speaker you want to clone\n",
    "path_stt_model=\"faster_whisper\" #path for the stt model use (must be a faster whisper model)\n",
    "lang=\"en\"\n",
    "\n",
    "## if you dont have enough memory you can lower embedding_batch_size=12,segmentation_batch_size=12,stt_batch_size=12\n",
    "\n",
    "datframe_speaker=get_matching_audio_speakers(wave,sp_rate,lst_speakers,HF_token,path_stt_model,lang,embedding_batch_size=12,segmentation_batch_size=12,stt_batch_size=12,num_speakers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dataset in the right format (create the directory, metadata.csv & wavs files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.finetuning import create_dataset\n",
    "## Path to where create the dataset\n",
    "path_dataset=\"train_data/\"\n",
    "\n",
    "## Max length for the audio size (I highly recommend to not play too much)\n",
    "max_audio_length=15\n",
    "\n",
    "create_dataset(wave,sp_rate,path_dataset,datframe_speaker,max_length=max_audio_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Too improve quality of the training set (to improve finetuning) you can check these ressources. https://github.com/coqui-ai/TTS/tree/dev/notebooks/dataset_analysis .\n",
    "### II) Training\n",
    "#### Most of parameters are recommended by https://docs.coqui.ai/\n",
    "#### You can modify all parameters in the utils/finetuning.py file but you need to know what you are doing.\n",
    "#### BATCH_SIZE\n",
    "- **Definition**: The `BATCH_SIZE` parameter defines the number of samples that are processed together in a single forward and backward pass during training.\n",
    "- **Value**: Default value `3`\n",
    "- **Explanation**: A small batch size like 3 means that the model weights are updated more frequently per epoch, which can lead to a more stable and possibly faster convergence in some cases. However, it can also make training more sensitive to noise in the data and computationally expensive in terms of time.\n",
    "\n",
    "#### GRAD_ACUMM_STEPS\n",
    "- **Definition**: `GRAD_ACUMM_STEPS` stands for gradient accumulation steps. It is the number of steps for which gradients are accumulated before performing a weight update.\n",
    "- **Value**: Default value `84`\n",
    "- **Explanation**: This parameter effectively simulates a larger batch size by accumulating gradients over multiple mini-batches. After 84 steps of accumulation, a weight update is performed. This approach can help stabilize training when using small batch sizes, reducing the variance in gradient updates and potentially leading to better model performance.\n",
    "\n",
    "#### nb_epochs\n",
    "- **Definition**: The `nb_epochs` parameter indicates the number of complete passes through the entire training dataset.\n",
    "- **Value**: Default value `5`\n",
    "- **Explanation**: With 10 epochs, the model will iterate over the entire training dataset ten times. This allows the model to learn from the data iteratively, refining its weights over each pass. The number of epochs should be chosen based on the model's convergence behavior and the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.finetuning import create_model_args, create_audio_config, create_trainer_config\n",
    "\n",
    "base_model_path=\"xtts_v2/\"\n",
    "model_args=create_model_args(base_model_path,max_audio_length,sp_rate)\n",
    "audio_config=create_audio_config(sp_rate)\n",
    "\n",
    "## Where to store the new model\n",
    "OUT_PATH=\"finetuning_xtts\" \n",
    "\n",
    "BATCH_SIZE = 3\n",
    "GRAD_ACUMM_STEPS = 84\n",
    "nb_epochs=5\n",
    "\n",
    "config=create_trainer_config(model_args,audio_config,OUT_PATH,BATCH_SIZE,nb_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training! This can take long time, espcially depending on how much vram you can allocate, don't hesitate to analyze the logs to know how the model is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.finetuning import training\n",
    "training(config,lang,path_dataset,GRAD_ACUMM_STEPS,OUT_PATH,base_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III) Inference\n",
    "#### The inference of the finetuning it's the same as the base model. Let's compare the two audios generated.\n",
    "#### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REFERENCE AUDIO come from the dataset\n",
    "from utils.tts_pipeline import generation,load_model\n",
    "import IPython\n",
    "import gc\n",
    "import torch \n",
    "\n",
    "ref_speaker=path_dataset+\"/wavs/audio1.wav\"\n",
    "## Load the model\n",
    "model=load_model(base_model_path)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "if device == \"cuda\":\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "gpt_cond_latent, speaker_embedding = model.get_conditioning_latents(audio_path=ref_speaker)\n",
    "generation(model,\"I'm Elon Musk, this is my first audio.\",\"en\",gpt_cond_latent,speaker_embedding,\"test.wav\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "del model\n",
    "IPython.display.Audio(\"test.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model(OUT_PATH)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "if device == \"cuda\":\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "gpt_cond_latent, speaker_embedding = model.get_conditioning_latents(audio_path=ref_speaker)\n",
    "generation(model,\"I'm Elon Musk, this is my first audio.\",\"en\",gpt_cond_latent,speaker_embedding,\"test_ft.wav\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "del model\n",
    "IPython.display.Audio(\"test_ft.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV) How to improve results\n",
    "- **One shot voice cloning** : Use audio reference with one speaker, no artifacts (music, background, noises...), best quality and play with parameters like temperature, top_k...\n",
    "\n",
    "- **Speech-to-text** : Use audio with no artifacts (music, background, noises...), use SOTA model (whisper-large-v3 & pyannote vad) and use speech alignment like WhisperX to have better timestamps.\n",
    "\n",
    "- **Diarization** : Use numbers of speakers_parameters, use SOTA model (pyannote). You can also implement the min_number_speaker and max number_speakers if you don't know the exact number.\n",
    "- **Dataset Creation** : Better audio chunks splitting, completes sentences, better audio length distribution. Do dataset analysis and clean the outlier in the dataset (https://github.com/coqui-ai/TTS/tree/dev/notebooks/dataset_analysis).\n",
    "- **Finetuning** : Try differents parameters (more epochs), plot the different losses to analyze the learning curve, more quality data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VoiceCloner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
